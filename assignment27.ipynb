{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcea64b2-e020-4722-a92b-93ede8551dc4",
   "metadata": {},
   "source": [
    "# Ans1-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e56cb5-972c-4ca3-a92a-e8d6214dff5f",
   "metadata": {},
   "source": [
    "R-squared in linear regression represents the proportion of the variance in the dependent variable that is explained by the independent variable(s); it is calculated as the square of the correlation between observed and predicted values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e627f1e-cf0d-432f-abfb-f01e0165ebde",
   "metadata": {},
   "source": [
    "# Ans2-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9686b7ed-84cf-45c5-95af-969f1ec3e19f",
   "metadata": {},
   "source": [
    "\n",
    "Adjusted R-squared penalizes the inclusion of irrelevant predictors in a model, providing a more accurate measure of goodness-of-fit by accounting for the number of predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5b45a9-4f8d-482f-896b-b0bbf05ed7f9",
   "metadata": {},
   "source": [
    "# Ans3-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8e3561-0a75-45f2-86cc-cd2559bb5140",
   "metadata": {},
   "source": [
    "Adjusted R-squared is more appropriate when comparing models with different numbers of predictors, helping to account for potential overfitting and providing a more reliable measure of goodness-of-fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f476d319-155e-419c-be46-e859cd1a53b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared: 0.20489153849835406\n",
      "Adjusted R-squared: 0.19677818685037818\n"
     ]
    }
   ],
   "source": [
    "# for example:\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Generate some random data\n",
    "np.random.seed(42)\n",
    "x = np.random.rand(100, 1)\n",
    "y = 2 * x + 1 + np.random.randn(100, 1)\n",
    "\n",
    "# Add a constant term to the independent variable (for the intercept)\n",
    "x_with_constant = sm.add_constant(x)\n",
    "\n",
    "# Fit the linear regression model\n",
    "model = sm.OLS(y, x_with_constant).fit()\n",
    "\n",
    "# Get R-squared and adjusted R-squared\n",
    "r_squared = model.rsquared\n",
    "adjusted_r_squared = model.rsquared_adj\n",
    "\n",
    "print(f'R-squared: {r_squared}')\n",
    "print(f'Adjusted R-squared: {adjusted_r_squared}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01270d1d-5b19-462c-91b1-18815a595f2a",
   "metadata": {},
   "source": [
    "# Ans4-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84909ad8-68c9-46c3-981d-e49aacff676d",
   "metadata": {},
   "source": [
    "RMSE (Root Mean Squared Error): Square root of the average squared differences between predicted and actual values; emphasizes larger errors.\n",
    "\n",
    "MSE (Mean Squared Error): Average of squared differences between predicted and actual values; penalizes larger errors.\n",
    "\n",
    "MAE (Mean Absolute Error): Average of absolute differences between predicted and actual values; robust to outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c3786c-9ee2-4441-8139-854ee6bcd0d0",
   "metadata": {},
   "source": [
    "# Ans5-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7fd3c1-cea3-4641-b207-68e148d11fd8",
   "metadata": {},
   "source": [
    "RMSE (Root Mean Squared Error):\n",
    "\n",
    "Advantage: Sensitivity to large errors provides a balanced view.\n",
    "\n",
    "Disadvantage: Sensitive to outliers.\n",
    "\n",
    "MSE (Mean Squared Error):\n",
    "\n",
    "Advantage: Emphasizes larger errors, useful for penalizing outliers.\n",
    "\n",
    "Disadvantage: Not in the same scale as the original data.\n",
    "\n",
    "MAE (Mean Absolute Error):\n",
    "\n",
    "Advantage: Robust to outliers, easy to interpret.\n",
    "\n",
    "Disadvantage: Ignores error distribution, less sensitive to extreme values.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2c672d-808f-43b5-a659-3ee2c7363bcc",
   "metadata": {},
   "source": [
    "# Ans6-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169a56d3-3f57-498a-a70b-7520b8f8bfd7",
   "metadata": {},
   "source": [
    "\n",
    "Lasso regularization adds the absolute values of the regression coefficients as a penalty term, \n",
    "\n",
    "Ridge regularization by promoting some coefficients to exactly zero, acting as feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1812535-3356-46ad-bb80-dd1cc11c3e3e",
   "metadata": {},
   "source": [
    "Use Lasso when feature selection is essential or when dealing with a dataset with many irrelevant features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4d6be0-0638-45b8-a5f0-f340a844ca46",
   "metadata": {},
   "source": [
    "# Ans7-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3368090a-f1d1-405c-bc4c-41ac0bbb169b",
   "metadata": {},
   "source": [
    "\n",
    "Regularized linear models penalize large coefficients, preventing them from fitting the noise in the training data, thereby reducing model complexity and overfitting.\n",
    "\n",
    "Example: In Ridge regression, the regularization term (L2 penalty) limits the size of the coefficients, preventing them from becoming too large"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1292663c-0c3a-4c7b-b84f-bdb2e03f8fd7",
   "metadata": {},
   "source": [
    "# Ans8-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7989f035-d1f1-4961-a374-4b133f033506",
   "metadata": {},
   "source": [
    "\n",
    "Limitations: Regularized linear models may not perform well when the underlying relationship between predictors and response is highly nonlinear or when there are interactions between variables that regularization cannot capture effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dce9f4-b4b4-493e-83e3-2d5e6e8b9b8a",
   "metadata": {},
   "source": [
    "# Ans9-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376b2e1d-6f12-4f37-9c68-f56ce8fba104",
   "metadata": {},
   "source": [
    "\n",
    "The choice depends on the specific goals and characteristics of the problem. If preferring a metric that emphasizes larger errors, Model A (RMSE) might be chosen. If prioritizing a metric robust to outliers, Model B (MAE) is preferable. However, limitations exist; for instance, RMSE is sensitive to outliers, and MAE might underemphasize the impact of larger errors. The decision should consider the context and priorities of the application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072136f5-2481-4f8d-95ae-6ad910465877",
   "metadata": {},
   "source": [
    "# Ans10-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a15c6f-6993-4aa0-a79c-d5ddc4bd79c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
